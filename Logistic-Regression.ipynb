{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d278174-f41a-455e-9cc1-4e5e13fba633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the NumPy, and few other libraries for utility functions\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e8c661-5b5a-4c7d-ab59-7798bf2cefac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a make_classification object to setup the Binary classification problem\n",
    "# Data-set size: 1 million, 50 dimensional instances, of which only 10 are informative\n",
    "# Other parameters: 5 redundant features (linear combinations of useful features), \n",
    "#                   weights = 0.7 (class-porportions),\n",
    "#                   class_sep = 0.5 (spread of cluster)\n",
    "\n",
    "X, y = make_classification(n_samples=1000000, n_features=50, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90b844e9-de29-47e1-bd42-e1214c0194d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 50)\n",
      "(1000000,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of instances, and their corresponding labels\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08095b3f-0540-4612-a6cd-def083f88f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a StandardScaler object to transform the data-set to mean equal to zero, and\n",
    "# standard deviation equal to 1.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e59342c3-3676-4eea-9377-a192bc64b8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.311607757605089e-16\n",
      "1.0000000000000016\n"
     ]
    }
   ],
   "source": [
    "# Printing out the mean, and standard deviation for the transform data-set.\n",
    "\n",
    "print(X.mean())\n",
    "print(X.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891a5a6a-702a-4fd0-a7f6-9897ceb79f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a train_test_split object to break the data set into 75% training data, and\n",
    "# 25% test data.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d786f3-7ce7-447e-bb7e-b0e915827d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750000, 50)\n",
      "(750000,)\n",
      "(250000, 50)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "# Printing out the shapes of training, and testing instances, along with their corresponding labels.\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceaf9296-7f7c-4ffe-a160-6e5a9a487329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard implementation of the Sigmoid function\n",
    "# The Sigmoid function maps the value into the range [0, 1].\n",
    "\n",
    "def sigmoid(X_train, weights):\n",
    "    unbounded_predictions = np.dot(X_train, weights)\n",
    "    return 1 / (1 + np.exp(-unbounded_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cd76e8f-91a0-479d-b750-49a94a56fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized implementation of the Gradient Descent algorithm\n",
    "\n",
    "def gradient_descent(X_train, y_train):\n",
    "    # Appending an extra column of 1s into the training data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    # Initializing the weights with random values\n",
    "    weights = np.random.random(X_train.shape[1])\n",
    "        \n",
    "    # Initializing the learning rate\n",
    "    learning_rate = 0.01\n",
    "        \n",
    "    # Restricting the number of iterations to 1000\n",
    "    n_iters = 1000\n",
    "    for _ in range(n_iters):\n",
    "        # Making a prediction for the current value of weights\n",
    "        predictions = sigmoid(X_train, weights)\n",
    "        \n",
    "        # Calculating the losses for every instance in the training data\n",
    "        losses = y_train - predictions\n",
    "        \n",
    "        # Calculating the gradient using the losses\n",
    "        gradients = np.dot(X_train.T, losses) / X_train.shape[0]\n",
    "                \n",
    "        # Updating the weights\n",
    "        weights += learning_rate * gradients\n",
    "            \n",
    "    # Returning the estimated weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69c672cc-4ed3-4c3b-b8a1-63680843cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions for the testing data using the estimated weights\n",
    "\n",
    "def make_prediction(X_test, weights):\n",
    "    # Appending an extra column of 1s into the testing data\n",
    "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "    \n",
    "    # Making a prediction using the estimated weights\n",
    "    predictions = sigmoid(X_test, weights)\n",
    "    \n",
    "    # Setting the threshold to 0.5 for class predictions\n",
    "    class_zero = (predictions < 0.50)  \n",
    "    \n",
    "    # Mapping values in the range [0, 1] to their corresponding classes\n",
    "    predictions[class_zero] = 0\n",
    "    predictions[~class_zero] = 1\n",
    "    \n",
    "    # Returing the predicted classes\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbdaadb5-c36b-45a0-9d2a-c128ea03445a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Estimating the weights using Gradient Descent algorithm\n",
    "# Timing the cell to evaluate the running time, and benchmarking with other variants\n",
    "\n",
    "weights = gradient_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dc89ba6-e8fc-4a7a-bb87-6e8eb856f882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "0.728972\n"
     ]
    }
   ],
   "source": [
    "# Making the predictions on the testing data, and printing the result\n",
    "\n",
    "predictions = make_prediction(X_test, weights)\n",
    "print(predictions)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd17f9d4-feea-445a-a365-9c42b8ae991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized implementation of the Stochastic Gradient Descent algorithm\n",
    "\n",
    "def stochastic_gradient_descent(X_train, y_train):\n",
    "    # Appending an extra column of 1s into the training data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    # Initializing the weights with random values\n",
    "    weights = np.random.random(X_train.shape[1])\n",
    "    \n",
    "    # Initializing the learning rate\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Restricting the number of iterations to 1000\n",
    "    n_iters = 1000\n",
    "    for _ in range(n_iters):\n",
    "        # Picking a random index in the valid index range\n",
    "        idx = np.random.randint(0, X_train.shape[0], size=1)\n",
    "        \n",
    "        # Picking the corresponding instance, and it's label\n",
    "        X_sampled = X_train[idx, :]\n",
    "        y_sampled = y_train[idx]\n",
    "        \n",
    "        # Making a prediction for the current value of weights\n",
    "        predictions = sigmoid(X_sampled, weights)\n",
    "        \n",
    "        # Calculating the loss for only the picked instance\n",
    "        loss = y_sampled - predictions\n",
    "        \n",
    "        # Calculating the gradient using the loss\n",
    "        gradient = np.dot(X_sampled.T, loss)\n",
    "        \n",
    "        # Updating the weights\n",
    "        weights += learning_rate * gradient \n",
    "    \n",
    "    # Returning the estimated weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25109970-e95d-4e26-9d75-8b9c3181f634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 163 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Estimating the weights using Stochastic Gradient Descent algorithm\n",
    "# Timing the cell to evaluate the running time, and benchmarking with other variants\n",
    "\n",
    "weights = stochastic_gradient_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17854de4-2bd8-44b2-8644-69f97ea584cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "0.721708\n"
     ]
    }
   ],
   "source": [
    "# Making the predictions on the testing data, and printing the result\n",
    "\n",
    "predictions = make_prediction(X_test, weights)\n",
    "print(predictions)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e96b6d3c-667d-495d-a696-838433df9a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized implementation of the Mini-batch Gradient Descent algorithm\n",
    "\n",
    "def mini_batch_gradient_descent(X_train, y_train):\n",
    "    # Appending an extra column of 1s into the training data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    # Initializing the weights with random values\n",
    "    weights = np.random.random(X_train.shape[1])\n",
    "    \n",
    "    # Initializing the learning rate\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Restricting the number of iterations to 1000\n",
    "    n_iters = 1000\n",
    "    \n",
    "    # Setting the batch size to 1 / 100th the size of the training data\n",
    "    sample_size = X_train.shape[0] // 100\n",
    "    for _ in range(n_iters):\n",
    "        # Picking random indexes in the valid index range\n",
    "        idx = np.random.choice(X_train.shape[0], size=sample_size, replace=False)\n",
    "        \n",
    "        # Picking the corresponding instances, and their labels to create a batch\n",
    "        X_sampled = X_train[idx, :]\n",
    "        y_sampled = y_train[idx]\n",
    "        \n",
    "        # Making a prediction for the current value of weights\n",
    "        predictions = sigmoid(X_sampled, weights)\n",
    "        \n",
    "        # Calculating the losses for the batch\n",
    "        losses = y_sampled - predictions\n",
    "        \n",
    "        # Calculating the gradients using the losses\n",
    "        gradients = np.dot(X_sampled.T, losses) / X_sampled.shape[0]\n",
    "        \n",
    "        # Updating the weights\n",
    "        weights += learning_rate * gradients\n",
    "    \n",
    "    # Returning the estimated weights\n",
    "    return weights        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad339665-66f0-4487-8b96-fffe0534b8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Estimating the weights using Mini-batch Gradient Descent algorithm\n",
    "# Timing the cell to evaluate the running time, and benchmarking with other variants\n",
    "\n",
    "weights = mini_batch_gradient_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b3650fe-d771-4077-8ba5-71d58138e051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 1. 1.]\n",
      "0.732008\n"
     ]
    }
   ],
   "source": [
    "# Making the predictions on the testing data, and printing the result\n",
    "\n",
    "predictions = make_prediction(X_test, weights)\n",
    "print(predictions)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e88a63-0687-477b-b1ef-fbd1e9e03888",
   "metadata": {},
   "source": [
    "| Algorithm | Training time | Accuracy Score         \n",
    "| :- |-------------: | :-:\n",
    "|Gradient Decent| 1 min 3 sec  | 0.7289\n",
    "| Stochastic Gradient Descent | 163 ms | 0.7217\n",
    "| Mini-Batch Gradient Descent | 26.7 sec | 0.7320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c82da6-6a65-4023-9686-61e34f93c3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
