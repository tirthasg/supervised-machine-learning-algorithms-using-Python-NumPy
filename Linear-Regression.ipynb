{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2ba36b-b572-41ef-8bdc-8b0fcc1b9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the NumPy, and few other libraries for utility functions\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf4eec0b-3f3b-45e3-8103-899e35370ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a make_regression object to setup the Regression problem\n",
    "# Data-set size: 1 million, 50 dimensional instances, of which only 10 are informative\n",
    "# Other parameters: noise = 2 (The std of Gaussian noise added to the output)\n",
    "\n",
    "X, y = make_regression(n_samples=1000000, n_features=50, n_informative=10, noise=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab7b57b5-5307-4856-bd8b-e6bf472f2a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 50)\n",
      "(1000000,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of instances, and their corresponding targets\n",
    "\n",
    "print(X.shape) \n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c98f9f3c-0f95-4516-93d2-ccaa3567ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a StandardScaler object to transform the data-set to mean equal to zero, and\n",
    "# standard deviation equal to 1.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab3e7aa6-6811-4cb7-8e66-fcf88f9ac992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1572077457676642e-18\n",
      "0.9999999999999958\n"
     ]
    }
   ],
   "source": [
    "# Printing out the mean, and standard deviation for the transform data-set.\n",
    "\n",
    "print(X.mean())\n",
    "print(X.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adb6d587-8033-413c-9b25-c2e360aec94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a train_test_split object to break the data set into 75% training data, and\n",
    "# 25% test data.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "783d493d-7dd5-49d2-9bf6-2fcfb64b2b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750000, 50)\n",
      "(750000,)\n",
      "(250000, 50)\n",
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "# Printing out the shapes of training, and testing instances, along with their corresponding labels.\n",
    "\n",
    "print(X_train.shape) \n",
    "print(y_train.shape) \n",
    "print(X_test.shape) \n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7ca9b63-b150-4ab5-afaa-676e5c91c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized implementation of the Gradient Descent algorithm\n",
    "\n",
    "def gradient_descent(X_train, y_train):\n",
    "    # Appending an extra column of 1s into the training data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    # Initializing the weights with random values\n",
    "    weights = np.random.random(X_train.shape[1])\n",
    "        \n",
    "    # Initializing the learning rate\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Restricting the number of iterations to 1000\n",
    "    n_iters = 1000\n",
    "    for _ in range(n_iters):\n",
    "        # Making a prediction for the current value of weights\n",
    "        predictions = np.dot(X_train, weights) \n",
    "        \n",
    "        # Calculating the losses for every instance in the training data\n",
    "        losses = y_train - predictions\n",
    "                \n",
    "        # Calculating the gradient using the losses\n",
    "        gradients = np.dot(X_train.T, losses) / X_train.shape[0]\n",
    "        \n",
    "        # Updating the weights\n",
    "        weights += learning_rate * gradients\n",
    "            \n",
    "    # Returning the estimated weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3efb791f-76a0-4fe4-886b-b4d34fa5c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions for the testing data using the estimated weights\n",
    "\n",
    "def make_prediction(X_test, weights):\n",
    "    # Appending an extra column of 1s into the testing data\n",
    "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "    \n",
    "    # Making a prediction using the estimated weights\n",
    "    predictions = np.dot(X_test, weights)\n",
    "    \n",
    "    # Returing the predicted targets\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca9ebb9b-cd4c-4ee8-ae77-e8355e45845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 40.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Estimating the weights using Gradient Descent algorithm\n",
    "# Timing the cell to evaluate the running time, and benchmarking with other variants\n",
    "\n",
    "weights = gradient_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5bdc8de-c7ee-47e1-9a21-74adf036d8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  68.07738807  -66.5145115    66.4842945  ... -213.02143821  -69.47491553\n",
      " -420.83642945]\n",
      "3.9953282406388966\n"
     ]
    }
   ],
   "source": [
    "# Making the predictions on the testing data, and printing the result\n",
    "predictions = make_prediction(X_test, weights)\n",
    "print(predictions)\n",
    "print(mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4f7863b-8a62-4686-9012-2f44157ab5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized implementation of the Stochastic Gradient Descent algorithm\n",
    "\n",
    "def stochastic_gradient_descent(X_train, y_train):\n",
    "    # Appending an extra column of 1s into the training data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    # Initializing the weights with random values\n",
    "    weights = np.random.random(X_train.shape[1])\n",
    "    \n",
    "    # Initializing the learning rate\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Restricting the number of iterations to 1000\n",
    "    n_iters = 1000\n",
    "    for _ in range(n_iters):\n",
    "        # Picking a random index in the valid index range\n",
    "        idx = np.random.randint(X_train.shape[0], size=1)\n",
    "        \n",
    "        # Picking the corresponding instance, and it's target\n",
    "        X_sampled = X_train[idx, :]\n",
    "        y_sampled = y_train[idx]\n",
    "        \n",
    "        # Making a prediction for the current value of weights\n",
    "        prediction = np.dot(X_sampled, weights)\n",
    "        \n",
    "        # Calculating the loss for only the picked instance\n",
    "        loss = y_sampled - prediction\n",
    "        \n",
    "        # Calculating the gradient using the loss\n",
    "        gradient = np.dot(X_sampled.T, loss)\n",
    "        \n",
    "        # Updating the weights\n",
    "        weights += learning_rate * gradient \n",
    "    \n",
    "    # Returning the estimated weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c6e726d-da86-4d94-992a-5a4fa169af25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 176 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Estimating the weights using Stochastic Gradient Descent algorithm\n",
    "# Timing the cell to evaluate the running time, and benchmarking with other variants\n",
    "\n",
    "weights = stochastic_gradient_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f9b46ee-fb84-430c-b6b6-8ed1a5e6080a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  70.05574816  -68.27151893   65.44607325 ... -212.98002917  -69.36914014\n",
      " -421.48958033]\n",
      "5.342540781287521\n"
     ]
    }
   ],
   "source": [
    "# Making the predictions on the testing data, and printing the result\n",
    "\n",
    "predictions = make_prediction(X_test, weights)\n",
    "print(predictions)\n",
    "print(mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec262add-58a0-47e4-a858-099855f91b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized implementation of the Mini-batch Gradient Descent algorithm\n",
    "\n",
    "def mini_batch_gradient_descent(X_train, y_train):\n",
    "    # Appending an extra column of 1s into the training data\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    \n",
    "    # Initializing the weights with random values\n",
    "    weights = np.random.random(X_train.shape[1])\n",
    "    \n",
    "    # Initializing the learning rate\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Restricting the number of iterations to 1000\n",
    "    n_iters = 1000\n",
    "    \n",
    "    # Setting the batch size to 1 / 100th the size of the training data\n",
    "    sample_size = X_train.shape[0] // 100\n",
    "    for _ in range(n_iters):\n",
    "        # Picking random indexes in the valid index range\n",
    "        idx = np.random.choice(X_train.shape[0], size=sample_size, replace=False)\n",
    "\n",
    "        # Picking the corresponding instances, and their labels to create a batch\n",
    "        X_sampled = X_train[idx, :]\n",
    "        y_sampled = y_train[idx]\n",
    "        \n",
    "        # Making a prediction for the current value of weights\n",
    "        predictions = np.dot(X_sampled, weights)\n",
    "        \n",
    "        # Calculating the losses for the batch\n",
    "        losses = y_sampled - predictions\n",
    "        \n",
    "        # Calculating the gradients using the losses\n",
    "        gradients = np.dot(X_sampled.T, losses) / X_sampled.shape[0]\n",
    "        \n",
    "        # Updating the weights\n",
    "        weights += learning_rate * gradients\n",
    "     \n",
    "    # Returning the estimated weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b030f9fd-a964-4ad4-a2d2-f1de5c717b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Estimating the weights using Mini-batch Gradient Descent algorithm\n",
    "# Timing the cell to evaluate the running time, and benchmarking with other variants\n",
    "\n",
    "weights = mini_batch_gradient_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3b48c9f-225d-4c22-99f4-c61818656f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  68.09432523  -66.49869118   66.46791186 ... -213.01669194  -69.48411853\n",
      " -420.83670452]\n",
      "3.995530032524403\n"
     ]
    }
   ],
   "source": [
    "# Making the predictions on the testing data, and printing the result\n",
    "\n",
    "predictions = make_prediction(X_test, weights)\n",
    "print(predictions)\n",
    "print(mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895ae8b4-333c-451a-ab61-190a8ca05a21",
   "metadata": {},
   "source": [
    "| Algorithm | Training time | MSE         \n",
    "| :- |-------------: | :-:\n",
    "|Gradient Decent| 40.7 sec  | 3.995\n",
    "| Stochastic Gradient Descent | 176 ms | 5.342\n",
    "| Mini-Batch Gradient Descent | 24 sec | 3.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbcc2b3-d5ab-4af1-b87d-8b7942516ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
